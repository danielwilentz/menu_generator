{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Firstly, read in all the things necessary to run the model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataframe of clean dishes\n",
    "\n",
    "cleaned_data = pd.read_pickle('data/kaggle_and_reddit_dishes_no_spaces.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in seeds\n",
    "\n",
    "with open('flask/flask_app/pickles/patterns.pkl', 'rb') as f:\n",
    "    seeds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in unique characters\n",
    "\n",
    "with open('flask/flask_app/pickles/unique_chars.pkl', 'rb') as f:\n",
    "    chars = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in character to integer dictionary\n",
    "\n",
    "with open('flask/flask_app/pickles/char_to_int.pkl', 'rb') as f:\n",
    "    char_to_int = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Define and load model\n",
    "\n",
    "seq_len = 35\n",
    "n_vocab = 51\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(seq_len, n_vocab)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(n_vocab, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "filepath_current = \"flask/flask_app/pickles/weights-improvement-150-0.5860.hdf5\"\n",
    "model.load_weights(filepath_current)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions to run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define helper function to adjust by temperature:\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    np.seterr(divide = 'ignore') \n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to generate text\n",
    "def generate_dish(seeds, lstm_model, temp, seq_len, n_vocab, output_len,\n",
    "                 char_to_int_dict, chars):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate a dish based on a series of seeds and an LSTM model\n",
    "\n",
    "    seeds: list of strings (each being seq_length long)\n",
    "    lstm_model: model, the LSTM model to use\n",
    "    temp: int, the temperature hyperparameter for LSTM\n",
    "    seq_len: int, the length of the seed string\n",
    "    n_vocab: int, number of unique characters\n",
    "    output_len: int, length of string to output (number of characters to generate)\n",
    "    char_to_int_dict: dict to convert characters to integers\n",
    "    chars: list, sorted set of all possible characters\n",
    "    \"\"\"\n",
    "\n",
    "    # Select a random pattern from seeds (the list of all patterns) \n",
    "    # as a random seed\n",
    "    start_index = np.random.randint(0, len(seeds))\n",
    "    orig_seed = seeds[start_index]\n",
    "\n",
    "    # initialize dish as empty list\n",
    "    dish = []\n",
    "    seed = orig_seed\n",
    "\n",
    "    # generate characters one by one\n",
    "    for w in range(output_len):\n",
    "        sampled = np.zeros((1, seq_len, n_vocab))\n",
    "        for t, char in enumerate(seed):\n",
    "            sampled[0, t, char_to_int_dict[char]] = 1.\n",
    "\n",
    "\n",
    "        preds = lstm_model.predict(sampled, verbose=0)[0]\n",
    "        next_index = sample(preds, temp)\n",
    "        next_char = chars[next_index]\n",
    "\n",
    "        seed += next_char\n",
    "        seed = seed[1:]\n",
    "\n",
    "        dish.append(next_char)\n",
    "\n",
    "    dish_gen = \"\".join(dish)\n",
    "    return dish_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erved sweet potato chopsed in the cover and with meat or fish orders wide dotuto sauce steamed rice \n"
     ]
    }
   ],
   "source": [
    "output = generate_dish(seeds=seeds, lstm_model=model, temp=0.7, seq_len=seq_len, \n",
    "                       n_vocab=n_vocab, output_len=100, char_to_int_dict=char_to_int, chars=chars)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok how the hell are we gonna filter this thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'uce toasted baked potato peach chilli hot sour saute fill things mashed potatoes garlic mayonnaise d'"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'biscuit compote grapefruit old sauce french fried potatoes or sauce with cheese served on a'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get rid of half-words on the ends\n",
    "' '.join(output.split(' ')[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all words in my corpus\n",
    "\n",
    "all_text = ''\n",
    "\n",
    "for index, row in cleaned_data.iterrows():\n",
    "    all_text += row['cleaned_title'] + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = all_text.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = list(set(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle unique words:\n",
    "with open('flask/flask_app/pickles/unique_words.pkl', 'wb') as f:\n",
    "    pickle.dump(unique_words, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283711"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21766"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_output(raw_dish, avail_words):\n",
    "    \"\"\"\n",
    "    function to clean the output of my LSTM\n",
    "    \"\"\"\n",
    "    print(raw_dish)   \n",
    "    \n",
    "    bad_words = ['the', 'potat', 'nd', 'ad', 'comments', 'look', 'potatoe', 'read', 'er', 'us', 'review']\n",
    "    filler_words = ['and', 'of', 'with', 'or', 'a', 'choice', 'of', 'served', 'on', 'cooked', 'in', 'from']\n",
    "    \n",
    "    word_list = raw_dish.strip().split(' ')[1:]\n",
    "    cleaned_dish_list = []\n",
    "    \n",
    "    # Exclude any words not in the original corpus and in the \"bad words list\"\n",
    "    # EXCEPT, I allow one typo to make things more interesting\n",
    "    typos = 0\n",
    "    for word in word_list:\n",
    "        if word in avail_words and word not in bad_words:\n",
    "            cleaned_dish_list.append(word)\n",
    "        elif word not in avail_words:\n",
    "            if typos < 1:\n",
    "                cleaned_dish_list.append(word)\n",
    "            typos += 1\n",
    "    \n",
    "    # Limit to only one and/one or/one sauce:\n",
    "    num_and, num_or, num_sauce, num_potato, num_cheese = 0, 0, 0, 0, 0\n",
    "    sauce_lim = random.randint(1,2)\n",
    "    potato_lim = random.randint(1,2)\n",
    "    cheese_lim = random.randint(1,2)\n",
    "    \n",
    "    for i, word in enumerate(cleaned_dish_list):\n",
    "        if word == 'and':\n",
    "            num_and += 1\n",
    "        elif word == 'or':\n",
    "            num_or += 1\n",
    "        elif word == 'sauce':\n",
    "            num_sauce += 1\n",
    "        elif word == 'potato' or word == 'potatoes':\n",
    "            num_potato += 1\n",
    "        elif word == 'cheese':\n",
    "            num_cheese += 1\n",
    "        if num_and > 1 or num_or > 1 or num_sauce > sauce_lim or \\\n",
    "            num_potato > potato_lim or num_cheese > cheese_lim:\n",
    "            cleaned_dish_list = cleaned_dish_list[:i]\n",
    "    \n",
    "    # Make sure the dish doesn't end with filler words\n",
    "    while cleaned_dish_list[-1] in filler_words or len(cleaned_dish_list[-1]) <= 2:\n",
    "        cleaned_dish_list = cleaned_dish_list[:-1]\n",
    "    \n",
    "    # Make sure the dish doesn't start with filler words\n",
    "    while cleaned_dish_list[0] in filler_words:\n",
    "        cleaned_dish_list = cleaned_dish_list[1:]\n",
    "        \n",
    "    print(' '.join(cleaned_dish_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erved sweet potato chopsed in the cover and with meat or fish orders wide dotuto sauce steamed rice \n",
      "sweet potato chopsed in cover and with meat or fish orders wide sauce steamed rice\n"
     ]
    }
   ],
   "source": [
    "clean_output(output, unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
